{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source-to-parcel analysis\n",
    "\n",
    "# Import necessary libraries\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns  # required for heatmap visualization\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from mne.viz import circular_layout\n",
    "import pandas as pd\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as npc\n",
    "import cupy as np  # using gpu acceleration\n",
    "import cupyx.scipy.fft\n",
    "import mne\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from nilearn import datasets\n",
    "from nilearn.image import get_data\n",
    "from scipy.signal import hilbert  # scipy core modified in env, running custom lib\n",
    "import scipy\n",
    "import matplotlib\n",
    "import os.path as op\n",
    "\n",
    "matplotlib.use('Agg')  # Setting the backend BEFORE importing pyplot\n",
    "\n",
    "\n",
    "scipy.fft.set_backend(cupyx.scipy.fft)\n",
    "\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = op.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "subject = \"fsaverage\"\n",
    "trans = \"fsaverage\"  # MNE has a built-in fsaverage transformation\n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "\n",
    "\n",
    "# Import necessary Python modules\n",
    "matplotlib.use('Agg')  # disable plotting\n",
    "mne.viz.set_browser_backend('matplotlib', verbose=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'matplotlib')\n",
    "\n",
    "\n",
    "# defining input and output directory\n",
    "files_in = '../data/in/subjects/'\n",
    "files_out = '../data/out/subjects/'\n",
    "\n",
    "\n",
    "# loading list of subject names from txt file\n",
    "names = open(\"./names.txt\", \"r\")\n",
    "subject_list = names.read().split('\\n')\n",
    "modes = ['EC', 'EO']\n",
    "# Read the custom montage\n",
    "montage_path = r\"../data/in/MFPRL_UPDATED_V2.sfp\"\n",
    "montage = mne.channels.read_custom_montage(montage_path)\n",
    "\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100)\n",
    "fs_dir = '../data/in/fsaverage'\n",
    "fname = os.path.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "src = mne.read_source_spaces(fname, patch_stats=False, verbose=None)\n",
    "\n",
    "#need to gen the following\n",
    "# Participant ID\n",
    "# Group assignment (might need \n",
    "# @Maxine He\n",
    "#  to remind us one last time about how the numbering relates to the group assignment)}\n",
    "# Condition (Eyes-open or eyes-closed)\n",
    "# Modularity\n",
    "# Small-worldness\n",
    "# Global Efficiency\n",
    "# Average clustering coefficient\n",
    "# Average betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_correlation(data_window):\n",
    "    \"\"\"Compute cross-correlation for given data window.\"\"\"\n",
    "    # Reshape the data to be 2D\n",
    "\n",
    "    data_2D = data_window.reshape(data_window.shape[0], -1)\n",
    "    correlation_matrix = np.corrcoef(data_2D, rowvar=True)\n",
    "    return correlation_matrix\n",
    "\n",
    "    # Compute dPLI at the level of regions\n",
    "\n",
    "\n",
    "def compute_dPLI(data):\n",
    "    print('Computing dPLI')\n",
    "    n_regions = data.shape[1]  # Compute for regions\n",
    "    dPLI_matrix = np.zeros((n_regions, n_regions))\n",
    "    print(data)\n",
    "    analytic_signal = hilbert(data)\n",
    "    phase_data = np.angle(analytic_signal)\n",
    "    for i in range(n_regions):\n",
    "        for j in range(n_regions):\n",
    "            if i != j:\n",
    "                phase_diff = phase_data[:, i] - phase_data[:, j]\n",
    "                dPLI_matrix[i, j] = np.abs(\n",
    "                    np.mean(np.exp(complex(0, 1) * phase_diff)))\n",
    "    return dPLI_matrix\n",
    "\n",
    "# dPLI_matrix = compute_dPLI(label_time_courses) --> computing static, fc for the entire dataset\n",
    "\n",
    "\n",
    "def disparity_filter(G, alpha=0.01):\n",
    "    disparities = {}\n",
    "    for i, j, data in G.edges(data=True):\n",
    "        weight_sum_square = sum(\n",
    "            [d['weight']**2 for _, _, d in G.edges(i, data=True)])\n",
    "        disparities[(i, j)] = data['weight']**2 / weight_sum_square\n",
    "\n",
    "    G_filtered = G.copy()\n",
    "    for (i, j), disparity in disparities.items():\n",
    "        if disparity < alpha:\n",
    "            G_filtered.remove_edge(i, j)\n",
    "    return G_filtered\n",
    "\n",
    "\n",
    "def graph_to_matrix(graph, size):\n",
    "    matrix = np.zeros((size, size))\n",
    "    for i, j, data in graph.edges(data=True):\n",
    "        matrix[i, j] = data['weight']\n",
    "        matrix[j, i] = data['weight']  # Ensure symmetry\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def threshold_matrix(matrix):\n",
    "    G_temp = nx.convert_matrix.from_numpy_array(matrix)\n",
    "    G_temp_thresholded = disparity_filter(G_temp)\n",
    "\n",
    "    matrix_thresholded = np.zeros_like(matrix)\n",
    "    for i, j, data in G_temp_thresholded.edges(data=True):\n",
    "        matrix_thresholded[i, j] = data['weight']\n",
    "        matrix_thresholded[j, i] = data['weight']\n",
    "    return matrix_thresholded\n",
    "\n",
    "\n",
    "def threshold_graph_by_density(G, density=0.1, directed=False):\n",
    "    if density < 0 or density > 1:\n",
    "        raise ValueError(\"Density value must be between 0 and 1.\")\n",
    "    num_edges_desired = int(G.number_of_edges() * density)\n",
    "    sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'],\n",
    "                          reverse=True)\n",
    "    if directed:\n",
    "        G_thresholded = nx.DiGraph()\n",
    "    else:\n",
    "        G_thresholded = nx.Graph()\n",
    "    G_thresholded.add_edges_from(sorted_edges[:num_edges_desired])\n",
    "    return G_thresholded\n",
    "\n",
    "# Convert dPLI to PLI\n",
    "\n",
    "\n",
    "def dpli_to_pli(dpli_matrix):\n",
    "    return 2 * np.abs(dpli_matrix - 0.5)\n",
    "\n",
    "\n",
    "def compute_disparity(G):\n",
    "    \"\"\"\n",
    "    Compute the disparity Y(i,j) for each edge in the graph.\n",
    "    \"\"\"\n",
    "    disparities = {}\n",
    "    for i, j, data in G.edges(data=True):\n",
    "        weight_sum_square = sum(\n",
    "            [d['weight']**2 for _, _, d in G.edges(i, data=True)])\n",
    "        disparities[(i, j)] = data['weight']**2 / weight_sum_square\n",
    "    return disparities\n",
    "\n",
    "\n",
    "def threshold_graph_by_density(G, density=0.1, directed=False):\n",
    "    if density < 0 or density > 1:\n",
    "        raise ValueError(\"Density value must be between 0 and 1.\")\n",
    "    num_edges_desired = int(G.number_of_edges() * density)\n",
    "    sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'],\n",
    "                          reverse=True)\n",
    "    if directed:\n",
    "        G_thresholded = nx.DiGraph()\n",
    "    else:\n",
    "        G_thresholded = nx.Graph()\n",
    "    G_thresholded.add_edges_from(sorted_edges[:num_edges_desired])\n",
    "    return G_thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subject_list:\n",
    "    for mode in modes:\n",
    "        print(subject, mode)\n",
    "        # defining paths for current subject\n",
    "        input_path = files_in+subject + '/' + mode + '/'\n",
    "        output_path = files_out + subject + '/' + mode + '/'\n",
    "\n",
    "        # loading in time course files\n",
    "\n",
    "        label_time_courses_file = output_path + \\\n",
    "            f\"{subject}_label_time_courses.npy\"\n",
    "        label_time_courses = np.load(label_time_courses_file)\n",
    "\n",
    "        labels = mne.read_labels_from_annot('fsaverage', parc='Schaefer2018_100Parcels_7Networks_order',\n",
    "                                            subjects_dir=r'../data/in/')\n",
    "\n",
    "        # Sampling rate and window parameters\n",
    "        sampling_rate = 512  # in Hz\n",
    "        window_length_seconds = 1  # desired window length in seconds\n",
    "        step_size_seconds = 0.5  # desired step size in seconds\n",
    "\n",
    "        # Convert time to samples\n",
    "        window_length_samples = int(\n",
    "            window_length_seconds * sampling_rate)  # convert to samples\n",
    "        step_size_samples = int(\n",
    "            step_size_seconds * sampling_rate)  # convert to samples\n",
    "\n",
    "        # Calculate total duration in samples\n",
    "        num_epochs_per_hemisphere = label_time_courses.shape[0] / 2\n",
    "        duration_per_epoch = label_time_courses.shape[2] / sampling_rate\n",
    "        total_duration_samples = int(\n",
    "            num_epochs_per_hemisphere * duration_per_epoch * sampling_rate)\n",
    "\n",
    "        # Time-resolved dPLI computation\n",
    "        num_windows = int(\n",
    "            (total_duration_samples - window_length_samples) / step_size_samples) + 1\n",
    "        windowed_dpli_matrices = []\n",
    "        windowed_pli_matrices = []\n",
    "\n",
    "        # Compute dPLI for each window\n",
    "        for win_idx in range(num_windows):\n",
    "            start_sample = win_idx * step_size_samples\n",
    "            end_sample = start_sample + window_length_samples\n",
    "\n",
    "            # Check if end_sample exceeds the total number of samples\n",
    "            if end_sample > total_duration_samples:\n",
    "                break\n",
    "\n",
    "            # Calculate epoch and sample indices\n",
    "            start_epoch = start_sample // label_time_courses.shape[2]\n",
    "            start_sample_in_epoch = start_sample % label_time_courses.shape[2]\n",
    "            end_epoch = end_sample // label_time_courses.shape[2]\n",
    "            end_sample_in_epoch = end_sample % label_time_courses.shape[2]\n",
    "\n",
    "            # Extract data across epochs\n",
    "            if start_epoch == end_epoch:\n",
    "                windowed_data = label_time_courses[start_epoch,\n",
    "                                                :, start_sample_in_epoch:end_sample_in_epoch]\n",
    "            else:\n",
    "                first_part = label_time_courses[start_epoch, :, start_sample_in_epoch:]\n",
    "                samples_needed_from_second_epoch = window_length_samples - \\\n",
    "                    first_part.shape[1]\n",
    "                second_part = label_time_courses[end_epoch,\n",
    "                                                :, :samples_needed_from_second_epoch]\n",
    "                windowed_data = np.concatenate(\n",
    "                    (first_part, second_part), axis=1)  # Change axis back to 1\n",
    "\n",
    "            dpli_result = compute_dPLI(windowed_data)\n",
    "            pli_result = dpli_to_pli(dpli_result)  # Convert dPLI to PLI\n",
    "            windowed_dpli_matrices.append(dpli_result)\n",
    "            windowed_pli_matrices.append(pli_result)\n",
    "        \n",
    "        # Check the number of windows in the list\n",
    "        num_of_windows = len(windowed_dpli_matrices)\n",
    "        print(f\"Total number of windows: {num_of_windows}\")\n",
    "\n",
    "        # Construct Directed Graphs\n",
    "        G_dPLI = nx.from_numpy_array(dpli_result, create_using=nx.DiGraph)\n",
    "        G_PLI = nx.from_numpy_array(pli_result, create_using=nx.Graph)\n",
    "\n",
    "        G_dPLI_thresholded = threshold_graph_by_density(G_dPLI)\n",
    "        G_PLI_thresholded = threshold_graph_by_density(G_PLI)\n",
    "\n",
    "        if not nx.is_strongly_connected(G_dPLI_thresholded):\n",
    "            largest_scc = max(nx.strongly_connected_components(\n",
    "                G_dPLI_thresholded), key=len)\n",
    "            G_dPLI_thresholded = G_dPLI_thresholded.subgraph(largest_scc).copy()\n",
    "\n",
    "        if not nx.is_connected(G_PLI_thresholded):\n",
    "            largest_cc = max(nx.connected_components(G_PLI_thresholded), key=len)\n",
    "            G_PLI_thresholded = G_PLI_thresholded.subgraph(largest_cc).copy()\n",
    "\n",
    "        # Calculate edge density for dPLI and PLI thresholded graphs\n",
    "        p_dPLI = len(G_dPLI_thresholded.edges()) / (G_dPLI_thresholded.number_of_nodes()\n",
    "                                                    * (G_dPLI_thresholded.number_of_nodes() - 1))\n",
    "        p_PLI = len(G_PLI_thresholded.edges()) / (G_PLI_thresholded.number_of_nodes()\n",
    "                                                * (G_PLI_thresholded.number_of_nodes() - 1))\n",
    "\n",
    "        # Compute graph theoretical metrics for dPLI\n",
    "        modularity_dPLI = nx.algorithms.community.modularity(G_dPLI_thresholded,\n",
    "                                                            nx.algorithms.community.greedy_modularity_communities(\n",
    "                                                                G_dPLI_thresholded))\n",
    "        clustering_coefficient_dPLI = nx.average_clustering(G_dPLI_thresholded)\n",
    "        avg_path_length_dPLI = nx.average_shortest_path_length(G_dPLI_thresholded)\n",
    "\n",
    "        # Convert directed graph to undirected for global efficiency calculation\n",
    "        G_dPLI_undirected = G_dPLI_thresholded.to_undirected()\n",
    "        global_efficiency_dPLI = nx.global_efficiency(G_dPLI_undirected)\n",
    "\n",
    "        betweenness_dict_dPLI = nx.betweenness_centrality(G_dPLI_thresholded)\n",
    "        avg_betweenness_dPLI = sum(\n",
    "            betweenness_dict_dPLI.values()) / len(betweenness_dict_dPLI)\n",
    "\n",
    "        # Compute graph theoretical metrics for PLI\n",
    "        modularity_PLI = nx.algorithms.community.modularity(G_PLI_thresholded,\n",
    "                                                            nx.algorithms.community.greedy_modularity_communities(\n",
    "                                                                G_PLI_thresholded))\n",
    "        clustering_coefficient_PLI = nx.average_clustering(G_PLI_thresholded)\n",
    "        avg_path_length_PLI = nx.average_shortest_path_length(G_PLI_thresholded)\n",
    "        global_efficiency_PLI = nx.global_efficiency(G_PLI_thresholded)\n",
    "        betweenness_dict_PLI = nx.betweenness_centrality(G_PLI_thresholded)\n",
    "        avg_betweenness_PLI = sum(betweenness_dict_PLI.values()\n",
    "                                ) / len(betweenness_dict_PLI)\n",
    "\n",
    "        # Small-worldness for dPLI\n",
    "        C_rand_dPLI = nx.average_clustering(nx.erdos_renyi_graph(\n",
    "            G_dPLI_thresholded.number_of_nodes(), p_dPLI, directed=True))\n",
    "        L_rand_dPLI = nx.average_shortest_path_length(nx.erdos_renyi_graph(\n",
    "            G_dPLI_thresholded.number_of_nodes(), p_dPLI, directed=True))\n",
    "        small_worldness_dPLI = (clustering_coefficient_dPLI /\n",
    "                                C_rand_dPLI) / (avg_path_length_dPLI / L_rand_dPLI)\n",
    "\n",
    "        # Small-worldness for PLI\n",
    "        C_rand_PLI = nx.average_clustering(nx.erdos_renyi_graph(\n",
    "            G_PLI_thresholded.number_of_nodes(), p_PLI))\n",
    "        L_rand_PLI = nx.average_shortest_path_length(\n",
    "            nx.erdos_renyi_graph(G_PLI_thresholded.number_of_nodes(), p_PLI))\n",
    "        small_worldness_PLI = (clustering_coefficient_PLI /\n",
    "                            C_rand_PLI) / (avg_path_length_PLI / L_rand_PLI)\n",
    "\n",
    "        # Display computed metrics for dPLI\n",
    "        print(f\"Metrics for dPLI:\")\n",
    "        print(f\"Modularity: {modularity_dPLI}\")\n",
    "        print(f\"Small-Worldness: {small_worldness_dPLI}\")\n",
    "        print(f\"Global Efficiency: {global_efficiency_dPLI}\")\n",
    "        print(f\"Average Clustering Coefficient: {clustering_coefficient_dPLI}\")\n",
    "        print(f\"Average Betweenness Centrality: {avg_betweenness_dPLI}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Display computed metrics for PLI\n",
    "        print(f\"Metrics for PLI:\")\n",
    "        print(f\"Modularity: {modularity_PLI}\")\n",
    "        print(f\"Small-Worldness: {small_worldness_PLI}\")\n",
    "        print(f\"Global Efficiency: {global_efficiency_PLI}\")\n",
    "        print(f\"Average Clustering Coefficient: {clustering_coefficient_PLI}\")\n",
    "        print(f\"Average Betweenness Centrality: {avg_betweenness_PLI}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win_idx in range(num_windows):\n",
    "    start_sample = win_idx * window_length_samples\n",
    "    end_sample = start_sample + window_length_samples\n",
    "\n",
    "    if end_sample > total_duration_samples:\n",
    "        break\n",
    "\n",
    "    # Extract the data for the current window\n",
    "    windowed_data = label_time_courses[:, :, start_sample:end_sample]\n",
    "\n",
    "    # Sub-divide the window into two halves\n",
    "    first_half = windowed_data[:, :, :window_length_samples // 2]\n",
    "    second_half = windowed_data[:, :, window_length_samples // 2:]\n",
    "\n",
    "    # Perform computations on each half\n",
    "    for half_data in [first_half, second_half]:\n",
    "        # wPLI computation\n",
    "        wpli_result = compute_wPLI(half_data)\n",
    "        windowed_wpli_matrices.append(wpli_result)\n",
    "\n",
    "        # Amplitude coupling computation\n",
    "        amp_coupling_result = compute_amplitude_coupling(half_data, labels)\n",
    "        windowed_amplitude_coupling.append(amp_coupling_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
