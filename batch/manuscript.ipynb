{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source-to-parcel analysis\n",
    "\n",
    "# Import necessary libraries\n",
    "from matplotlib.animation import FuncAnimation\n",
    "# import seaborn as sns  # required for heatmap visualization\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from mne.viz import circular_layout\n",
    "import pandas as pd\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as npc\n",
    "import cupy as np  # using gpu acceleration\n",
    "import cupyx.scipy.fft\n",
    "import mne\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from nilearn import datasets\n",
    "from nilearn.image import get_data\n",
    "from scipy.signal import hilbert  # scipy core modified in env, running custom lib\n",
    "import scipy\n",
    "import matplotlib\n",
    "import os.path as op\n",
    "\n",
    "matplotlib.use('Agg')  # Setting the backend BEFORE importing pyplot\n",
    "\n",
    "\n",
    "scipy.fft.set_backend(cupyx.scipy.fft)\n",
    "\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = op.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "subject = \"fsaverage\"\n",
    "trans = \"fsaverage\"  # MNE has a built-in fsaverage transformation\n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "\n",
    "\n",
    "# Import necessary Python modules\n",
    "matplotlib.use('Agg')  # disable plotting\n",
    "mne.viz.set_browser_backend('matplotlib', verbose=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'matplotlib')\n",
    "\n",
    "\n",
    "# defining input and output directory\n",
    "files_in = '../data/in/subjects/'\n",
    "files_out = '../data/out/subjects/'\n",
    "\n",
    "\n",
    "def compute_cross_correlation(data_window):\n",
    "    \"\"\"Compute cross-correlation for given data window.\"\"\"\n",
    "    # Reshape the data to be 2D\n",
    "\n",
    "    data_2D = data_window.reshape(data_window.shape[0], -1)\n",
    "    correlation_matrix = np.corrcoef(data_2D, rowvar=True)\n",
    "    return correlation_matrix\n",
    "\n",
    "    # Compute dPLI at the level of regions\n",
    "\n",
    "\n",
    "def compute_dPLI(data):\n",
    "    print('Computing dPLI')\n",
    "    n_regions = data.shape[1]  # Compute for regions\n",
    "    dPLI_matrix = np.zeros((n_regions, n_regions))\n",
    "    print(data)\n",
    "    analytic_signal = hilbert(data)\n",
    "    phase_data = np.angle(analytic_signal)\n",
    "    for i in range(n_regions):\n",
    "        for j in range(n_regions):\n",
    "            if i != j:\n",
    "                phase_diff = phase_data[:, i] - phase_data[:, j]\n",
    "                dPLI_matrix[i, j] = np.abs(\n",
    "                    np.mean(np.exp(complex(0, 1) * phase_diff)))\n",
    "    return dPLI_matrix\n",
    "\n",
    "# dPLI_matrix = compute_dPLI(label_time_courses) --> computing static, fc for the entire dataset\n",
    "\n",
    "\n",
    "def disparity_filter(G, alpha=0.01):\n",
    "    disparities = {}\n",
    "    for i, j, data in G.edges(data=True):\n",
    "        weight_sum_square = sum(\n",
    "            [d['weight']**2 for _, _, d in G.edges(i, data=True)])\n",
    "        disparities[(i, j)] = data['weight']**2 / weight_sum_square\n",
    "\n",
    "    G_filtered = G.copy()\n",
    "    for (i, j), disparity in disparities.items():\n",
    "        if disparity < alpha:\n",
    "            G_filtered.remove_edge(i, j)\n",
    "    return G_filtered\n",
    "\n",
    "\n",
    "def graph_to_matrix(graph, size):\n",
    "    matrix = np.zeros((size, size))\n",
    "    for i, j, data in graph.edges(data=True):\n",
    "        matrix[i, j] = data['weight']\n",
    "        matrix[j, i] = data['weight']  # Ensure symmetry\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def plot_matrix(matrix, title, labels, cmap='viridis'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # plt.imsave(matrix, cmap='autumn')\n",
    "    sns.heatmap(matrix, cmap=cmap, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.savefig(output_path+title+'.png')\n",
    "\n",
    "\n",
    "def update(window_number):\n",
    "    ax.clear()\n",
    "    current_matrix = graph_to_matrix(nx.convert_matrix.from_numpy_array(\n",
    "        windowed_cross_correlation_matrices[window_number]), windowed_cross_correlation_matrices[window_number].shape[0])\n",
    "    sns.heatmap(current_matrix, cmap='viridis',\n",
    "                xticklabels=ordered_regions, yticklabels=ordered_regions, ax=ax)\n",
    "    title.set_text(\n",
    "        f'Thresholded Cross-Correlation Matrix for Window {window_number}')\n",
    "    return ax\n",
    "\n",
    "\n",
    "def update(window_number):\n",
    "    ax.clear()\n",
    "    current_matrix = threshold_matrix(windowed_dpli_matrices[window_number])\n",
    "    plot_connectivity_circle(current_matrix, ordered_regions, n_lines=300, node_angles=node_angles,\n",
    "                             title=f'Thresholded Regional Connectivity using dPLI for Window {window_number}', ax=ax)\n",
    "    return ax,\n",
    "\n",
    "\n",
    "def threshold_matrix(matrix):\n",
    "    G_temp = nx.convert_matrix.from_numpy_array(matrix)\n",
    "    G_temp_thresholded = disparity_filter(G_temp)\n",
    "\n",
    "    matrix_thresholded = np.zeros_like(matrix)\n",
    "    for i, j, data in G_temp_thresholded.edges(data=True):\n",
    "        matrix_thresholded[i, j] = data['weight']\n",
    "        matrix_thresholded[j, i] = data['weight']\n",
    "    return matrix_thresholded\n",
    "\n",
    "\n",
    "def threshold_graph_by_density(G, density=0.1, directed=False):\n",
    "    if density < 0 or density > 1:\n",
    "        raise ValueError(\"Density value must be between 0 and 1.\")\n",
    "    num_edges_desired = int(G.number_of_edges() * density)\n",
    "    sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'],\n",
    "                          reverse=True)\n",
    "    if directed:\n",
    "        G_thresholded = nx.DiGraph()\n",
    "    else:\n",
    "        G_thresholded = nx.Graph()\n",
    "    G_thresholded.add_edges_from(sorted_edges[:num_edges_desired])\n",
    "    return G_thresholded\n",
    "\n",
    "\n",
    "# loading list of subject names from txt file\n",
    "names = open(\"./names.txt\", \"r\")\n",
    "subject_list = names.read().split('\\n')\n",
    "modes = ['EC', 'EO']\n",
    "# Read the custom montage\n",
    "montage_path = r\"../data/in/MFPRL_UPDATED_V2.sfp\"\n",
    "montage = mne.channels.read_custom_montage(montage_path)\n",
    "\n",
    "\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100)\n",
    "fs_dir = '../data/in/fsaverage'\n",
    "fname = os.path.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "src = mne.read_source_spaces(fname, patch_stats=False, verbose=None)\n",
    "\n",
    "#need to gen the following\n",
    "# Participant ID\n",
    "# Group assignment (might need \n",
    "# @Maxine He\n",
    "#  to remind us one last time about how the numbering relates to the group assignment)}\n",
    "# Condition (Eyes-open or eyes-closed)\n",
    "# Modularity\n",
    "# Small-worldness\n",
    "# Global Efficiency\n",
    "# Average clustering coefficient\n",
    "# Average betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subject_list:\n",
    "    for mode in modes:\n",
    "        print(subject, mode)\n",
    "        # defining paths for current subject\n",
    "        input_path = files_in+subject + '/' + mode + '/'\n",
    "        output_path = files_out + subject + '/' + mode + '/'\n",
    "\n",
    "        # loading in time course files\n",
    "\n",
    "        label_time_courses_file = output_path + \\\n",
    "            f\"{subject}_label_time_courses.npy\"\n",
    "        label_time_courses = np.load(label_time_courses_file)\n",
    "\n",
    "        labels = mne.read_labels_from_annot('fsaverage', parc='Schaefer2018_100Parcels_7Networks_order',\n",
    "                                            subjects_dir=r'../data/in/')\n",
    "\n",
    "        # Group labels by network\n",
    "        networks = {}\n",
    "        for label in labels:\n",
    "            # Extract network name from label name (assuming format: 'NetworkName_RegionName')\n",
    "            network_name = label.name.split('_')[0]\n",
    "            if network_name not in networks:\n",
    "                networks[network_name] = []\n",
    "            networks[network_name].append(label)\n",
    "\n",
    "           # Organize regions by their network affiliations and extract the desired naming convention\n",
    "        ordered_regions = []\n",
    "        network_labels = []  # This will store the network each region belongs to\n",
    "\n",
    "        for label in labels:\n",
    "            # Extract the desired naming convention \"PFCl_1-lh\" from the full label name\n",
    "            parts = label.name.split('_')\n",
    "            region_name = '_'.join(parts[2:])\n",
    "            ordered_regions.append(region_name)\n",
    "\n",
    "            # Extract the network name and store it in network_labels\n",
    "            network_name = parts[2]\n",
    "            network_labels.append(network_name)\n",
    "\n",
    "            # Compute cross-correlation between all pairs of regions across windows\n",
    "            print('Computing cross corelation')\n",
    "            # Time-resolved dPLI computation\n",
    "            sampling_rate = 512  # in Hz\n",
    "            window_length_seconds = 1\n",
    "            step_size_seconds = 0.5\n",
    "\n",
    "            # Total duration in samples\n",
    "            # Assuming the structure is the same as label_time_courses in Code 2\n",
    "            num_epochs_per_hemisphere = label_time_courses.shape[0] / 2\n",
    "            duration_per_epoch = label_time_courses.shape[2] / sampling_rate\n",
    "            total_duration_samples = int(\n",
    "                num_epochs_per_hemisphere * duration_per_epoch * sampling_rate)\n",
    "\n",
    "            window_length_samples = int(window_length_seconds * sampling_rate)\n",
    "            step_size_samples = int(step_size_seconds * sampling_rate)\n",
    "\n",
    "            num_windows = int(\n",
    "                (total_duration_samples - window_length_samples) / step_size_samples) + 1\n",
    "            windowed_dpli_matrices = []\n",
    "            windowed_cross_correlation_matrices = []\n",
    "\n",
    "        print(networks)\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
