{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cupy as np\n",
    "import scipy\n",
    "import networkx as nx\n",
    "\n",
    "import mne.viz\n",
    "import mne_connectivity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as npy\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from nilearn import datasets\n",
    "from nilearn.image import get_data\n",
    "from scipy.signal import hilbert  # scipy core modified in env, running custom lib\n",
    "import scipy\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "matplotlib.use('Agg')  # Setting the backend BEFORE importing pyplot\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary Python modules\n",
    "matplotlib.use('Agg')  # disable plotting\n",
    "mne.viz.set_browser_backend('matplotlib', verbose=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'matplotlib')\n",
    "\n",
    "\n",
    "# defining input and output directory\n",
    "files_in = '../data/in/subjects/'\n",
    "files_out = '../data/out/subjects/'\n",
    "\n",
    "\n",
    "# The files live in:\n",
    "subject = \"fsaverage\"\n",
    "trans = \"fsaverage\"  # MNE has a built-in fsaverage transformation\n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "\n",
    "\n",
    "names = open(\"./names.txt\", \"r\")\n",
    "subject_list = names.read().split('\\n')\n",
    "modes = ['EC', 'EO']\n",
    "\n",
    "\n",
    "# Participant ID\n",
    "# Group assignment (might need \n",
    "# @Maxine He\n",
    "#  to remind us one last time about how the numbering relates to the group assignment)}\n",
    "# Condition (Eyes-open or eyes-closed)\n",
    "# Modularity\n",
    "# Small-worldness\n",
    "# Global Efficiency\n",
    "# Average clustering coefficient\n",
    "# Average betweenness centrality\n",
    "\n",
    "dict_struct = {\n",
    "    'PID': None,\n",
    "    'Group': None,\n",
    "    'Condition': None,\n",
    "    'Modularity': None,\n",
    "    'Small-Worldedness': None,\n",
    "    'Global Efficiency': None,\n",
    "    'Average clustering coefficient': None,\n",
    "    'Average betweenness centrality': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Definitions\n",
    "def compute_cross_correlation(data_window):\n",
    "    \"\"\"Compute cross-correlation for given data window.\"\"\"\n",
    "    # Reshape the data to be 2D\n",
    "\n",
    "    data_2D = data_window.reshape(data_window.shape[0], -1)\n",
    "    correlation_matrix = np.corrcoef(data_2D, rowvar=True)\n",
    "    return correlation_matrix\n",
    "\n",
    "    # Compute dPLI at the level of regions\n",
    "\n",
    "\n",
    "def compute_dPLI(data):\n",
    "    print('Computing dPLI')\n",
    "    n_regions = data.shape[1]  # Compute for regions\n",
    "    dPLI_matrix = np.zeros((n_regions, n_regions))\n",
    "    print(data)\n",
    "    analytic_signal = hilbert(data)\n",
    "    phase_data = np.angle(analytic_signal)\n",
    "    for i in range(n_regions):\n",
    "        for j in range(n_regions):\n",
    "            if i != j:\n",
    "                phase_diff = phase_data[:, i] - phase_data[:, j]\n",
    "                dPLI_matrix[i, j] = np.abs(\n",
    "                    np.mean(np.exp(complex(0, 1) * phase_diff)))\n",
    "    return dPLI_matrix\n",
    "\n",
    "# dPLI_matrix = compute_dPLI(label_time_courses) --> computing static, fc for the entire dataset\n",
    "\n",
    "\n",
    "def disparity_filter(G, alpha=0.01):\n",
    "    disparities = {}\n",
    "    for i, j, data in G.edges(data=True):\n",
    "        weight_sum_square = sum(\n",
    "            [d['weight']**2 for _, _, d in G.edges(i, data=True)])\n",
    "        disparities[(i, j)] = data['weight']**2 / weight_sum_square\n",
    "\n",
    "    G_filtered = G.copy()\n",
    "    for (i, j), disparity in disparities.items():\n",
    "        if disparity < alpha:\n",
    "            G_filtered.remove_edge(i, j)\n",
    "    return G_filtered\n",
    "\n",
    "\n",
    "def graph_to_matrix(graph, size):\n",
    "    matrix = np.zeros((size, size))\n",
    "    for i, j, data in graph.edges(data=True):\n",
    "        matrix[i, j] = data['weight']\n",
    "        matrix[j, i] = data['weight']  # Ensure symmetry\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def plot_matrix(matrix, title, labels, cmap='viridis'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # plt.imsave(matrix, cmap='autumn')\n",
    "    sns.heatmap(matrix, cmap=cmap, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.savefig(output_path+title+'.png')\n",
    "\n",
    "\n",
    "def update(window_number):\n",
    "    ax.clear()\n",
    "    current_matrix = graph_to_matrix(nx.convert_matrix.from_numpy_array(\n",
    "        windowed_cross_correlation_matrices[window_number]), windowed_cross_correlation_matrices[window_number].shape[0])\n",
    "    sns.heatmap(current_matrix, cmap='viridis',\n",
    "                xticklabels=ordered_regions, yticklabels=ordered_regions, ax=ax)\n",
    "    title.set_text(\n",
    "        f'Thresholded Cross-Correlation Matrix for Window {window_number}')\n",
    "    return ax\n",
    "\n",
    "\n",
    "def update(window_number):\n",
    "    ax.clear()\n",
    "    current_matrix = threshold_matrix(windowed_dpli_matrices[window_number])\n",
    "    plot_connectivity_circle(current_matrix, ordered_regions, n_lines=300, node_angles=node_angles,\n",
    "                             title=f'Thresholded Regional Connectivity using dPLI for Window {window_number}', ax=ax)\n",
    "    return ax,\n",
    "\n",
    "\n",
    "def threshold_matrix(matrix):\n",
    "    G_temp = nx.convert_matrix.from_numpy_array(matrix)\n",
    "    G_temp_thresholded = disparity_filter(G_temp)\n",
    "\n",
    "    matrix_thresholded = np.zeros_like(matrix)\n",
    "    for i, j, data in G_temp_thresholded.edges(data=True):\n",
    "        matrix_thresholded[i, j] = data['weight']\n",
    "        matrix_thresholded[j, i] = data['weight']\n",
    "    return matrix_thresholded\n",
    "\n",
    "\n",
    "def threshold_graph_by_density(G, density=0.1, directed=False):\n",
    "    if density < 0 or density > 1:\n",
    "        raise ValueError(\"Density value must be between 0 and 1.\")\n",
    "    num_edges_desired = int(G.number_of_edges() * density)\n",
    "    sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'],\n",
    "                          reverse=True)\n",
    "    if directed:\n",
    "        G_thresholded = nx.DiGraph()\n",
    "    else:\n",
    "        G_thresholded = nx.Graph()\n",
    "    G_thresholded.add_edges_from(sorted_edges[:num_edges_desired])\n",
    "    return G_thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subject_list:\n",
    "    for mode in modes:\n",
    "        print(subject, mode)\n",
    "        # defining paths for current subject\n",
    "        input_path = files_in+subject + '/' + mode + '/'\n",
    "        output_path = files_out + subject + '/' + mode + '/'\n",
    "\n",
    "        stc_path = output_path + 'stc/'\n",
    "\n",
    "        inverse_solution_files_lh = []\n",
    "        inverse_solution_files_rh = []\n",
    "\n",
    "        for path, subdirs, files in os.walk(stc_path):\n",
    "            for file in files:\n",
    "                filepath = path + file\n",
    "                if '-rh.stc' in file:\n",
    "                    inverse_solution_files_rh.append(filepath)\n",
    "                elif '-lh.stc' in file:\n",
    "                    inverse_solution_files_lh.append(filepath)\n",
    "\n",
    "        # Error here !!!\n",
    "\n",
    "        # Calculate the total number of inverse solution files for both hemispheres\n",
    "        total_files_lh = len(inverse_solution_files_lh)\n",
    "        total_files_rh = len(inverse_solution_files_rh)\n",
    "\n",
    "        results = dict_struct\n",
    "        results['PID'] = subject\n",
    "        results['Condition'] = mode\n",
    "        results['Group'] = 'YA' if '1' in subject[0] else 'OA'\n",
    "\n",
    "        # Calculate the batch size for both hemispheres\n",
    "        # Change 10 to your desired batch size\n",
    "        batch_size_lh = total_files_lh // (total_files_lh // 10)\n",
    "\n",
    "        # Change 10 to your desired batch size\n",
    "        batch_size_rh = total_files_rh // (total_files_rh // 10)\n",
    "\n",
    "        # Ensure batch size is a multiple of 10 (or your desired batch size) for both hemispheres\n",
    "        while total_files_lh % batch_size_lh != 0:\n",
    "            batch_size_lh -= 1\n",
    "\n",
    "        while total_files_rh % batch_size_rh != 0:\n",
    "            batch_size_rh -= 1\n",
    "\n",
    "        # Initialize lists to store source estimates for both hemispheres\n",
    "        stcs_lh = []\n",
    "        stcs_rh = []\n",
    "        # Load data in batches for both hemispheres\n",
    "        for i in range(0, total_files_lh, batch_size_lh):\n",
    "            batch_files_lh = inverse_solution_files_lh[i:i + batch_size_lh]\n",
    "            batch_files_rh = inverse_solution_files_rh[i:i + batch_size_rh]\n",
    "\n",
    "            for file_path_lh, file_path_rh in zip(batch_files_lh, batch_files_rh):\n",
    "                try:\n",
    "                    print(file_path_lh, file_path_rh)\n",
    "                    stc_epoch_lh = mne.read_source_estimate(file_path_lh)\n",
    "                    stc_epoch_rh = mne.read_source_estimate(file_path_rh)\n",
    "                    stcs_lh.append(stc_epoch_lh)\n",
    "                    stcs_rh.append(stc_epoch_rh)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error loading files {file_path_lh} or {file_path_rh}: {e}\")\n",
    "\n",
    "        # Load labels from the atlas\n",
    "        labels = mne.read_labels_from_annot('fsaverage', parc='Schaefer2018_100Parcels_7Networks_order',\n",
    "                                            subjects_dir=r'../data/in/')\n",
    "\n",
    "        # Extract label time courses for both hemispheres\n",
    "        label_time_courses = []  # Initialize a list to store label time courses\n",
    "\n",
    "        print(stcs_lh, stcs_rh)\n",
    "        for idx, (stc_lh, stc_rh) in enumerate(zip(stcs_lh, stcs_rh)):\n",
    "            try:\n",
    "                label_tc_lh = stc_lh.extract_label_time_course(\n",
    "                    labels, src=src, mode='mean_flip')\n",
    "                label_tc_rh = stc_rh.extract_label_time_course(\n",
    "                    labels, src=src, mode='mean_flip')\n",
    "                label_time_courses.extend([label_tc_lh, label_tc_rh])\n",
    "                print(src)\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error extracting label time courses for iteration {idx}: {e}\")\n",
    "        else:  # This block will execute if the for loop completes without encountering a break statement\n",
    "            print(\"All time courses have been successfully extracted!\")\n",
    "\n",
    "        # Convert label_time_courses to a NumPy array\n",
    "        label_time_courses_np = np.array(label_time_courses)\n",
    "\n",
    "        # If you prefer to save as a .csv file\n",
    "        # Convert to DataFrame and save as .csv\n",
    "        # label_time_courses_df = pd.DataFrame(label_time_courses_np)\n",
    "        # label_time_courses_df.to_csv(os.path.join(output_dir, f\"{subj}_label_time_courses.csv\"), index=False)\n",
    "\n",
    "        # Save the label time courses as a .npy file\n",
    "        # Replace with your desired output directory\n",
    "        label_time_courses_file = output_path + \\\n",
    "            f\"{subject}_label_time_courses.npy\"\n",
    "\n",
    "        np.save(label_time_courses_file, label_time_courses_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source-to-parcel analysis\n",
    "\n",
    "# Import necessary libraries\n",
    "from matplotlib.animation import FuncAnimation\n",
    "# import seaborn as sns  # required for heatmap visualization\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from mne.viz import circular_layout\n",
    "import pandas as pd\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as npc\n",
    "import cupy as np  # using gpu acceleration\n",
    "import cupyx.scipy.fft\n",
    "import mne\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from nilearn import datasets\n",
    "from nilearn.image import get_data\n",
    "from scipy.signal import hilbert  # scipy core modified in env, running custom lib\n",
    "import scipy\n",
    "import matplotlib\n",
    "import os.path as op\n",
    "\n",
    "matplotlib.use('Agg')  # Setting the backend BEFORE importing pyplot\n",
    "\n",
    "\n",
    "scipy.fft.set_backend(cupyx.scipy.fft)\n",
    "\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = op.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "subject = \"fsaverage\"\n",
    "trans = \"fsaverage\"  # MNE has a built-in fsaverage transformation\n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "\n",
    "\n",
    "# Import necessary Python modules\n",
    "matplotlib.use('Agg')  # disable plotting\n",
    "mne.viz.set_browser_backend('matplotlib', verbose=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'matplotlib')\n",
    "\n",
    "\n",
    "# defining input and output directory\n",
    "files_in = '../data/in/subjects/'\n",
    "files_out = '../data/out/subjects/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loading list of subject names from txt file\n",
    "names = open(\"./names.txt\", \"r\")\n",
    "subject_list = names.read().split('\\n')\n",
    "modes = ['EC', 'EO']\n",
    "# Read the custom montage\n",
    "montage_path = r\"../data/in/MFPRL_UPDATED_V2.sfp\"\n",
    "montage = mne.channels.read_custom_montage(montage_path)\n",
    "\n",
    "# Define the map of channel names using the provided keys\n",
    "ch_map = {'Ch1': 'Fp1', 'Ch2': 'Fz', 'Ch3': 'F3', 'Ch4': 'F7', 'Ch5': 'LHEye', 'Ch6': 'FC5',\n",
    "          # Setting FPz as GND so it matches montage\n",
    "          'Ch7': 'FC1', 'Ch8': 'C3', 'Ch9': 'T7', 'Ch10': 'GND', 'Ch11': 'CP5', 'Ch12': 'CP1',\n",
    "          'Ch13': 'Pz', 'Ch14': 'P3', 'Ch15': 'P7', 'Ch16': 'O1', 'Ch17': 'Oz', 'Ch18': 'O2',\n",
    "          'Ch19': 'P4', 'Ch20': 'P8', 'Ch21': 'Rmastoid', 'Ch22': 'CP6', 'Ch23': 'CP2', 'Ch24': 'Cz',\n",
    "          'Ch25': 'C4', 'Ch26': 'T8', 'Ch27': 'RHEye', 'Ch28': 'FC6', 'Ch29': 'FC2', 'Ch30': 'F4',\n",
    "          'Ch31': 'F8', 'Ch32': 'Fp2', 'Ch33': 'AF7', 'Ch34': 'AF3', 'Ch35': 'AFz', 'Ch36': 'F1',\n",
    "          'Ch37': 'F5', 'Ch38': 'FT7', 'Ch39': 'FC3', 'Ch40': 'FCz', 'Ch41': 'C1', 'Ch42': 'C5',\n",
    "          'Ch43': 'TP7', 'Ch44': 'CP3', 'Ch45': 'P1', 'Ch46': 'P5', 'Ch47': 'Lneck', 'Ch48': 'PO3',\n",
    "          'Ch49': 'POz', 'Ch50': 'PO4', 'Ch51': 'Rneck', 'Ch52': 'P6', 'Ch53': 'P2', 'Ch54': 'CPz',\n",
    "          'Ch55': 'CP4', 'Ch56': 'TP8', 'Ch57': 'C6', 'Ch58': 'C2', 'Ch59': 'FC4', 'Ch60': 'FT8',\n",
    "          'Ch61': 'F6', 'Ch62': 'F2', 'Ch63': 'AF4', 'Ch64': 'RVEye'}\n",
    "\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100)\n",
    "fs_dir = '../data/in/fsaverage'\n",
    "fname = os.path.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "src = mne.read_source_spaces(fname, patch_stats=False, verbose=None)\n",
    "\n",
    "for subject in subject_list:\n",
    "    for mode in modes:\n",
    "        print(subject, mode)\n",
    "        # defining paths for current subject\n",
    "        input_path = files_in+subject + '/' + mode + '/'\n",
    "        output_path = files_out + subject + '/' + mode + '/'\n",
    "\n",
    "        stc_path = output_path + 'stc/'\n",
    "\n",
    "        inverse_solution_files_lh = []\n",
    "        inverse_solution_files_rh = []\n",
    "\n",
    "        for path, subdirs, files in os.walk(stc_path):\n",
    "            for file in files:\n",
    "                filepath = path + file\n",
    "                if '-rh.stc' in file:\n",
    "                    inverse_solution_files_rh.append(filepath)\n",
    "                elif '-lh.stc' in file:\n",
    "                    inverse_solution_files_lh.append(filepath)\n",
    "\n",
    "        # Error here !!!\n",
    "\n",
    "        # Calculate the total number of inverse solution files for both hemispheres\n",
    "        total_files_lh = len(inverse_solution_files_lh)\n",
    "        total_files_rh = len(inverse_solution_files_rh)\n",
    "\n",
    "        # Calculate the batch size for both hemispheres\n",
    "        # Change 10 to your desired batch size\n",
    "        batch_size_lh = total_files_lh // (total_files_lh // 10)\n",
    "\n",
    "        # Change 10 to your desired batch size\n",
    "        batch_size_rh = total_files_rh // (total_files_rh // 10)\n",
    "\n",
    "        # Ensure batch size is a multiple of 10 (or your desired batch size) for both hemispheres\n",
    "        while total_files_lh % batch_size_lh != 0:\n",
    "            batch_size_lh -= 1\n",
    "\n",
    "        while total_files_rh % batch_size_rh != 0:\n",
    "            batch_size_rh -= 1\n",
    "\n",
    "        # Initialize lists to store source estimates for both hemispheres\n",
    "        stcs_lh = []\n",
    "        stcs_rh = []\n",
    "\n",
    "        # Load data in batches for both hemispheres\n",
    "        for i in range(0, total_files_lh, batch_size_lh):\n",
    "            batch_files_lh = inverse_solution_files_lh[i:i + batch_size_lh]\n",
    "            batch_files_rh = inverse_solution_files_rh[i:i + batch_size_rh]\n",
    "\n",
    "            for file_path_lh, file_path_rh in zip(batch_files_lh, batch_files_rh):\n",
    "                try:\n",
    "                    print(file_path_lh, file_path_rh)\n",
    "                    stc_epoch_lh = mne.read_source_estimate(file_path_lh)\n",
    "                    stc_epoch_rh = mne.read_source_estimate(file_path_rh)\n",
    "                    stcs_lh.append(stc_epoch_lh)\n",
    "                    stcs_rh.append(stc_epoch_rh)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error loading files {file_path_lh} or {file_path_rh}: {e}\")\n",
    "\n",
    "        # Load labels from the atlas\n",
    "        labels = mne.read_labels_from_annot('fsaverage', parc='Schaefer2018_100Parcels_7Networks_order',\n",
    "                                            subjects_dir=r'../data/in/')\n",
    "\n",
    "        # Extract label time courses for both hemispheres\n",
    "        label_time_courses = []  # Initialize a list to store label time courses\n",
    "\n",
    "        print(stcs_lh, stcs_rh)\n",
    "        for idx, (stc_lh, stc_rh) in enumerate(zip(stcs_lh, stcs_rh)):\n",
    "            try:\n",
    "                label_tc_lh = stc_lh.extract_label_time_course(\n",
    "                    labels, src=src, mode='mean_flip')\n",
    "                label_tc_rh = stc_rh.extract_label_time_course(\n",
    "                    labels, src=src, mode='mean_flip')\n",
    "                label_time_courses.extend([label_tc_lh, label_tc_rh])\n",
    "                print(src)\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error extracting label time courses for iteration {idx}: {e}\")\n",
    "        else:  # This block will execute if the for loop completes without encountering a break statement\n",
    "            print(\"All time courses have been successfully extracted!\")\n",
    "\n",
    "        # Convert label_time_courses to a NumPy array\n",
    "        label_time_courses_np = np.array(label_time_courses)\n",
    "\n",
    "        # If you prefer to save as a .csv file\n",
    "        # Convert to DataFrame and save as .csv\n",
    "        # label_time_courses_df = pd.DataFrame(label_time_courses_np)\n",
    "        # label_time_courses_df.to_csv(os.path.join(output_dir, f\"{subj}_label_time_courses.csv\"), index=False)\n",
    "\n",
    "        # Save the label time courses as a .npy file\n",
    "        # Replace with your desired output directory\n",
    "        label_time_courses_file = output_path + \\\n",
    "            f\"{subject}_label_time_courses.npy\"\n",
    "\n",
    "        np.save(label_time_courses_file, label_time_courses_np)\n",
    "\n",
    "        ########################################################################################################################\n",
    "\n",
    "#         # Plotting Time Courses\n",
    "#         random_idx = np.random.randint(len(label_time_courses))\n",
    "#         random_time_course = label_time_courses[random_idx]\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(random_time_course)\n",
    "#         plt.title(f'Time Course for Randomly Selected Region: {random_idx}')\n",
    "#         plt.xlabel('Time')\n",
    "#         plt.ylabel('Amplitude')\n",
    "\n",
    "\n",
    "#         plt.savefig(output_path+'time_courses.png')\n",
    "\n",
    "\n",
    "#         print('Connectivity Viz')\n",
    "#         # Connectivity Visualization for left hemisphere\n",
    "#         num_regions = len(label_time_courses[0])\n",
    "#         connectivity_matrix = np.zeros((num_regions, num_regions))\n",
    "\n",
    "#         for i in range(num_regions):\n",
    "#             for j in range(num_regions):\n",
    "#                 connectivity_matrix[i, j], _ = pearsonr(\n",
    "#                     label_time_courses[0][i], label_time_courses[0][j])\n",
    "\n",
    "#         plt.figure(figsize=(10, 10))\n",
    "#         plt.imshow(connectivity_matrix, cmap='viridis', origin='lower')\n",
    "#         plt.title('Connectivity Matrix')\n",
    "#         plt.xlabel('Region')\n",
    "#         plt.ylabel('Region')\n",
    "#         plt.colorbar(label='Pearson Correlation')\n",
    "\n",
    "#         print('Saving Connectivity Matrix')\n",
    "#         plt.savefig(output_path+'connectivity_matrix.png')\n",
    "\n",
    "#         # Average connectivity matrix across all epochs and hemispheres\n",
    "#         # Initialize connectivity matrix\n",
    "#         num_epochs_hemispheres = len(label_time_courses)\n",
    "#         num_regions = label_time_courses[0].shape[0]\n",
    "#         all_connectivity_matrices = np.zeros(\n",
    "#             (num_epochs_hemispheres, num_regions, num_regions))\n",
    "\n",
    "#         # Compute connectivity for each epoch and hemisphere\n",
    "#         print('Computing connectivity')\n",
    "#         print(num_epochs_hemispheres*num_regions*num_regions)\n",
    "#         for k in range(num_epochs_hemispheres):\n",
    "#             for i in range(num_regions):\n",
    "#                 for j in range(num_regions):\n",
    "#                     all_connectivity_matrices[k, i, j], _ = pearsonr(\n",
    "#                         label_time_courses[k][i], label_time_courses[k][j])\n",
    "\n",
    "#                     print(k,i,j)\n",
    "\n",
    "#         # Average across all epochs and hemispheres\n",
    "#         avg_connectivity_matrix = np.mean(all_connectivity_matrices, axis=0)\n",
    "\n",
    "#         # Visualization\n",
    "#         plt.figure(figsize=(10, 10))\n",
    "#         plt.imshow(avg_connectivity_matrix, cmap='viridis', origin='lower')\n",
    "#         plt.title('Average Connectivity Matrix')\n",
    "#         plt.xlabel('Region')\n",
    "#         plt.ylabel('Region')\n",
    "#         plt.colorbar(label='Pearson Correlation')\n",
    "\n",
    "#         print('saving avg pearson Connectivity Matrix')\n",
    "#         plt.savefig(output_path+'connectivity_avg_matrix.png')\n",
    "\n",
    "        ########################################################################################################################\n",
    "        # All-to-all connectivity analysis\n",
    "\n",
    "        # Load the label time courses\n",
    "        label_time_courses_file = output_path + \\\n",
    "            f\"{subject}_label_time_courses.npy\"\n",
    "\n",
    "        label_time_courses = np.load(label_time_courses_file)\n",
    "\n",
    "        # Load labels from the atlas\n",
    "        labels = mne.read_labels_from_annot('fsaverage', parc='Schaefer2018_100Parcels_7Networks_order',\n",
    "                                            subjects_dir=r'../data/in/')\n",
    "\n",
    "        # Group labels by network\n",
    "        networks = {}\n",
    "        for label in labels:\n",
    "            # Extract network name from label name (assuming format: 'NetworkName_RegionName')\n",
    "            network_name = label.name.split('_')[0]\n",
    "            if network_name not in networks:\n",
    "                networks[network_name] = []\n",
    "            networks[network_name].append(label)\n",
    "\n",
    "        # Organize regions by their network affiliations and extract the desired naming convention\n",
    "        ordered_regions = []\n",
    "        network_labels = []  # This will store the network each region belongs to\n",
    "\n",
    "        for label in labels:\n",
    "            # Extract the desired naming convention \"PFCl_1-lh\" from the full label name\n",
    "            parts = label.name.split('_')\n",
    "            region_name = '_'.join(parts[2:])\n",
    "            ordered_regions.append(region_name)\n",
    "\n",
    "            # Extract the network name and store it in network_labels\n",
    "            network_name = parts[2]\n",
    "            network_labels.append(network_name)\n",
    "\n",
    "        # Compute cross-correlation between all pairs of regions across windows\n",
    "        print('Computing cross corelation')\n",
    "        # Time-resolved dPLI computation\n",
    "        sampling_rate = 512  # in Hz\n",
    "        window_length_seconds = 1\n",
    "        step_size_seconds = 0.5\n",
    "\n",
    "        # Total duration in samples\n",
    "        # Assuming the structure is the same as label_time_courses in Code 2\n",
    "        num_epochs_per_hemisphere = label_time_courses.shape[0] / 2\n",
    "        duration_per_epoch = label_time_courses.shape[2] / sampling_rate\n",
    "        total_duration_samples = int(\n",
    "            num_epochs_per_hemisphere * duration_per_epoch * sampling_rate)\n",
    "\n",
    "        window_length_samples = int(window_length_seconds * sampling_rate)\n",
    "        step_size_samples = int(step_size_seconds * sampling_rate)\n",
    "\n",
    "        num_windows = int(\n",
    "            (total_duration_samples - window_length_samples) / step_size_samples) + 1\n",
    "        windowed_dpli_matrices = []\n",
    "        windowed_cross_correlation_matrices = []\n",
    "\n",
    "        for win_idx in range(num_windows):\n",
    "            print('Processing', win_idx)\n",
    "            start_sample = win_idx * step_size_samples\n",
    "            end_sample = start_sample + window_length_samples\n",
    "\n",
    "            # Check if end_sample exceeds the total number of samples\n",
    "            if end_sample > total_duration_samples:\n",
    "                break\n",
    "\n",
    "            # Calculate epoch and sample indices\n",
    "            start_epoch = start_sample // label_time_courses.shape[2]\n",
    "            start_sample_in_epoch = start_sample % label_time_courses.shape[2]\n",
    "            end_epoch = end_sample // label_time_courses.shape[2]\n",
    "            end_sample_in_epoch = end_sample % label_time_courses.shape[2]\n",
    "\n",
    "            # Extract data across epochs\n",
    "            if start_epoch == end_epoch:\n",
    "                windowed_data = label_time_courses[start_epoch,\n",
    "                                                   :, start_sample_in_epoch:end_sample_in_epoch]\n",
    "            else:\n",
    "                first_part = label_time_courses[start_epoch,\n",
    "                                                :, start_sample_in_epoch:]\n",
    "                samples_needed_from_second_epoch = window_length_samples - \\\n",
    "                    first_part.shape[1]\n",
    "                second_part = label_time_courses[end_epoch,\n",
    "                                                 :, :samples_needed_from_second_epoch]\n",
    "                windowed_data = np.concatenate(\n",
    "                    (first_part, second_part), axis=1)\n",
    "\n",
    "            # dPLI computation\n",
    "            dpli_result = compute_dPLI(windowed_data)\n",
    "            windowed_dpli_matrices.append(dpli_result)\n",
    "\n",
    "        # Check the number of windows in the list\n",
    "        num_of_windows = len(windowed_dpli_matrices)\n",
    "        print(f\"Total number of windows: {num_of_windows}\")\n",
    "\n",
    "        # Calculating the average connectivity across all windows (or choose a specific window)\n",
    "        # chosen_window = 0 # Choose a window\n",
    "        # can do windowed_dpli_matrices[chosen_window] to only choose one window\n",
    "        dPLI_matrix = windowed_dpli_matrices\n",
    "        G_dPLI_list = [nx.convert_matrix.from_numpy_array(\n",
    "            matrix) for matrix in windowed_dpli_matrices]\n",
    "\n",
    "        print('Working on alphas')\n",
    "        # Determining the optimal alpha value for disparity filter before applying thresholding\n",
    "\n",
    "        # Disparity filter\n",
    "\n",
    "        # Determining alpha\n",
    "        # Example range of alphas to test\n",
    "        alphas = np.linspace(0.001, 0.1, 100)\n",
    "        avg_connectivities = []\n",
    "\n",
    "        for alpha in alphas:\n",
    "            avg_conn_for_alpha = []\n",
    "            for G_dPLI in G_dPLI_list:  # Use the already converted graphs\n",
    "\n",
    "                print('Finding optimal alpha')\n",
    "                G_dPLI_thresholded = disparity_filter(G_dPLI, alpha=alpha)\n",
    "\n",
    "                # Convert the thresholded graph back to a matrix\n",
    "                dPLI_matrix_thresholded = nx.convert_matrix.to_numpy_array(\n",
    "                    G_dPLI_thresholded)\n",
    "\n",
    "                # Compute the average functional connectivity of the thresholded matrix\n",
    "                avg_conn = np.mean(dPLI_matrix_thresholded)\n",
    "                avg_conn_for_alpha.append(avg_conn)\n",
    "\n",
    "            # Compute the average of averages for this alpha\n",
    "            # AKA, for this alpha, what's the typical (or average) connectivity value across all windows?\n",
    "            avg_connectivities.append(np.mean(avg_conn_for_alpha))\n",
    "\n",
    "        # Find the alpha that gives the most stable average connectivity\n",
    "        optimal_alpha = alphas[np.argmin(np.diff(avg_connectivities))]\n",
    "\n",
    "        print(f\"Optimal alpha: {optimal_alpha}\")\n",
    "\n",
    "        # Thresholding the connectivity matrix\n",
    "        G_dPLI_thresholded_list = [threshold_graph_by_density(\n",
    "            G) for G in G_dPLI_list]  # Use the optimal alpha value here\n",
    "\n",
    "        ########################################################################################################################\n",
    "        # Next --> fc between groups and directed minimum spanning tree to examine nature of the fc difference between groups"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
