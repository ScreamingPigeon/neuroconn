{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source-to-parcel analysis\n",
    "\n",
    "# Import necessary libraries\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns  # required for heatmap visualization\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from mne.viz import circular_layout\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from nilearn import datasets\n",
    "from nilearn.image import get_data\n",
    "from scipy.signal import hilbert\n",
    "import matplotlib\n",
    "import os.path as op\n",
    "\n",
    "matplotlib.use('Agg')  # Setting the backend BEFORE importing pyplot\n",
    "\n",
    "fs_dir = fetch_fsaverage(verbose=True)\n",
    "subjects_dir = op.dirname(fs_dir)\n",
    "\n",
    "# The files live in:\n",
    "subject = \"fsaverage\"\n",
    "trans = \"fsaverage\"  # MNE has a built-in fsaverage transformation\n",
    "src = op.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "bem = op.join(fs_dir, \"bem\", \"fsaverage-5120-5120-5120-bem-sol.fif\")\n",
    "\n",
    "\n",
    "# Import necessary Python modules\n",
    "matplotlib.use('Agg')  # disable plotting\n",
    "mne.viz.set_browser_backend('matplotlib', verbose=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'matplotlib')\n",
    "\n",
    "\n",
    "# defining input and output directory\n",
    "files_in = '../data/in/subjects/'\n",
    "files_out = '../data/out/subjects/'\n",
    "\n",
    "\n",
    "def compute_cross_correlation(data_window):\n",
    "    \"\"\"Compute cross-correlation for given data window.\"\"\"\n",
    "    # Reshape the data to be 2D\n",
    "    data_2D = data_window.reshape(data_window.shape[0], -1)\n",
    "    correlation_matrix = np.corrcoef(data_2D, rowvar=True)\n",
    "    return correlation_matrix\n",
    "\n",
    "        # Compute dPLI at the level of regions\n",
    "\n",
    "\n",
    "def compute_dPLI(data):\n",
    "    n_regions = data.shape[1]  # Compute for regions\n",
    "    dPLI_matrix = np.zeros((n_regions, n_regions))\n",
    "    analytic_signal = hilbert(data)\n",
    "    phase_data = np.angle(analytic_signal)\n",
    "    for i in range(n_regions):\n",
    "        for j in range(n_regions):\n",
    "            if i != j:\n",
    "                phase_diff = phase_data[:, i] - phase_data[:, j]\n",
    "                dPLI_matrix[i, j] = np.abs(\n",
    "                    np.mean(np.exp(complex(0, 1) * phase_diff)))\n",
    "    return dPLI_matrix\n",
    "\n",
    "# dPLI_matrix = compute_dPLI(label_time_courses) --> computing static, fc for the entire dataset\n",
    "\n",
    "\n",
    "def disparity_filter(G, alpha=0.01):\n",
    "    disparities = {}\n",
    "    for i, j, data in G.edges(data=True):\n",
    "        weight_sum_square = sum(\n",
    "            [d['weight']**2 for _, _, d in G.edges(i, data=True)])\n",
    "        disparities[(i, j)] = data['weight']**2 / weight_sum_square\n",
    "\n",
    "    G_filtered = G.copy()\n",
    "    for (i, j), disparity in disparities.items():\n",
    "        if disparity < alpha:\n",
    "            G_filtered.remove_edge(i, j)\n",
    "    return G_filtered\n",
    "\n",
    "\n",
    "def graph_to_matrix(graph, size):\n",
    "    matrix = np.zeros((size, size))\n",
    "    for i, j, data in graph.edges(data=True):\n",
    "        matrix[i, j] = data['weight']\n",
    "        matrix[j, i] = data['weight']  # Ensure symmetry\n",
    "    return matrix\n",
    "\n",
    "def plot_matrix(matrix, title, labels, cmap='viridis'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(matrix, cmap=cmap, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.savefig(output_path+title+'.png')\n",
    "\n",
    "def update(window_number):\n",
    "    ax.clear()\n",
    "    current_matrix = graph_to_matrix(nx.convert_matrix.from_numpy_array(\n",
    "        windowed_cross_correlation_matrices[window_number]), windowed_cross_correlation_matrices[window_number].shape[0])\n",
    "    sns.heatmap(current_matrix, cmap='viridis',\n",
    "                xticklabels=ordered_regions, yticklabels=ordered_regions, ax=ax)\n",
    "    title.set_text(\n",
    "        f'Thresholded Cross-Correlation Matrix for Window {window_number}')\n",
    "    return ax\n",
    "\n",
    "def update(window_number):\n",
    "    ax.clear()\n",
    "    current_matrix = threshold_matrix(windowed_dpli_matrices[window_number])\n",
    "    plot_connectivity_circle(current_matrix, ordered_regions, n_lines=300, node_angles=node_angles,\n",
    "                            title=f'Thresholded Regional Connectivity using dPLI for Window {window_number}', ax=ax)\n",
    "    return ax,\n",
    "\n",
    "\n",
    "def threshold_matrix(matrix):\n",
    "    G_temp = nx.convert_matrix.from_numpy_array(matrix)\n",
    "    G_temp_thresholded = disparity_filter(G_temp)\n",
    "\n",
    "    matrix_thresholded = np.zeros_like(matrix)\n",
    "    for i, j, data in G_temp_thresholded.edges(data=True):\n",
    "        matrix_thresholded[i, j] = data['weight']\n",
    "        matrix_thresholded[j, i] = data['weight']\n",
    "    return matrix_thresholded\n",
    "\n",
    "\n",
    "# loading list of subject names from txt file\n",
    "names = open(\"./names.txt\", \"r\")\n",
    "subject_list = names.read().split('\\n')\n",
    "modes = ['EC', 'EO']\n",
    "# Read the custom montage\n",
    "montage_path = r\"../data/in/MFPRL_UPDATED_V2.sfp\"\n",
    "montage = mne.channels.read_custom_montage(montage_path)\n",
    "\n",
    "# Define the map of channel names using the provided keys\n",
    "ch_map = {'Ch1': 'Fp1', 'Ch2': 'Fz', 'Ch3': 'F3', 'Ch4': 'F7', 'Ch5': 'LHEye', 'Ch6': 'FC5',\n",
    "          # Setting FPz as GND so it matches montage\n",
    "          'Ch7': 'FC1', 'Ch8': 'C3', 'Ch9': 'T7', 'Ch10': 'GND', 'Ch11': 'CP5', 'Ch12': 'CP1',\n",
    "          'Ch13': 'Pz', 'Ch14': 'P3', 'Ch15': 'P7', 'Ch16': 'O1', 'Ch17': 'Oz', 'Ch18': 'O2',\n",
    "          'Ch19': 'P4', 'Ch20': 'P8', 'Ch21': 'Rmastoid', 'Ch22': 'CP6', 'Ch23': 'CP2', 'Ch24': 'Cz',\n",
    "          'Ch25': 'C4', 'Ch26': 'T8', 'Ch27': 'RHEye', 'Ch28': 'FC6', 'Ch29': 'FC2', 'Ch30': 'F4',\n",
    "          'Ch31': 'F8', 'Ch32': 'Fp2', 'Ch33': 'AF7', 'Ch34': 'AF3', 'Ch35': 'AFz', 'Ch36': 'F1',\n",
    "          'Ch37': 'F5', 'Ch38': 'FT7', 'Ch39': 'FC3', 'Ch40': 'FCz', 'Ch41': 'C1', 'Ch42': 'C5',\n",
    "          'Ch43': 'TP7', 'Ch44': 'CP3', 'Ch45': 'P1', 'Ch46': 'P5', 'Ch47': 'Lneck', 'Ch48': 'PO3',\n",
    "          'Ch49': 'POz', 'Ch50': 'PO4', 'Ch51': 'Rneck', 'Ch52': 'P6', 'Ch53': 'P2', 'Ch54': 'CPz',\n",
    "          'Ch55': 'CP4', 'Ch56': 'TP8', 'Ch57': 'C6', 'Ch58': 'C2', 'Ch59': 'FC4', 'Ch60': 'FT8',\n",
    "          'Ch61': 'F6', 'Ch62': 'F2', 'Ch63': 'AF4', 'Ch64': 'RVEye'}\n",
    "\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100)\n",
    "fs_dir = '../data/in/fsaverage'\n",
    "fname = os.path.join(fs_dir, \"bem\", \"fsaverage-ico-5-src.fif\")\n",
    "src = mne.read_source_spaces(fname, patch_stats=False, verbose=None)\n",
    "\n",
    "for subject in subject_list:\n",
    "    for mode in modes:\n",
    "        print(subject, mode)\n",
    "        # defining paths for current subject\n",
    "        input_path = files_in+subject + '/' + mode + '/'\n",
    "        output_path = files_out + subject + '/' + mode + '/'\n",
    "\n",
    "        stc_path  = output_path +'stc/'\n",
    "\n",
    "        inverse_solution_files_lh = []\n",
    "        inverse_solution_files_rh = []\n",
    "        \n",
    "        \n",
    "        for path, subdirs, files in os.walk(stc_path):\n",
    "            for file in files:\n",
    "                filepath = path + file\n",
    "                if '-rh.stc' in file:\n",
    "                    inverse_solution_files_rh.append(filepath)\n",
    "                elif '-lh.stc' in file:\n",
    "                    inverse_solution_files_lh.append(filepath)\n",
    "\n",
    "        # Error here !!!\n",
    "\n",
    "        # Calculate the total number of inverse solution files for both hemispheres\n",
    "        total_files_lh = len(inverse_solution_files_lh)\n",
    "        total_files_rh = len(inverse_solution_files_rh)\n",
    "\n",
    "        # Calculate the batch size for both hemispheres\n",
    "        # Change 10 to your desired batch size\n",
    "        batch_size_lh = total_files_lh // (total_files_lh // 10)\n",
    "\n",
    "        # Change 10 to your desired batch size\n",
    "        batch_size_rh = total_files_rh // (total_files_rh // 10)\n",
    "\n",
    "        # Ensure batch size is a multiple of 10 (or your desired batch size) for both hemispheres\n",
    "        while total_files_lh % batch_size_lh != 0:\n",
    "            batch_size_lh -= 1\n",
    "\n",
    "        while total_files_rh % batch_size_rh != 0:\n",
    "            batch_size_rh -= 1\n",
    "\n",
    "        # Initialize lists to store source estimates for both hemispheres\n",
    "        stcs_lh = []\n",
    "        stcs_rh = []\n",
    "\n",
    "        # Load data in batches for both hemispheres\n",
    "        for i in range(0, total_files_lh, batch_size_lh):\n",
    "            batch_files_lh = inverse_solution_files_lh[i:i + batch_size_lh]\n",
    "            batch_files_rh = inverse_solution_files_rh[i:i + batch_size_rh]\n",
    "\n",
    "            for file_path_lh, file_path_rh in zip(batch_files_lh, batch_files_rh):\n",
    "                try:\n",
    "                    print(file_path_lh, file_path_rh)\n",
    "                    stc_epoch_lh = mne.read_source_estimate(file_path_lh)\n",
    "                    stc_epoch_rh = mne.read_source_estimate(file_path_rh)\n",
    "                    stcs_lh.append(stc_epoch_lh)\n",
    "                    stcs_rh.append(stc_epoch_rh)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading files {file_path_lh} or {file_path_rh}: {e}\")\n",
    "\n",
    "        # Load labels from the atlas\n",
    "        labels = mne.read_labels_from_annot('fsaverage', parc='Schaefer2018_100Parcels_7Networks_order',\n",
    "                                            subjects_dir=r'../data/in/')\n",
    "\n",
    "        # Extract label time courses for both hemispheres\n",
    "        label_time_courses = []  # Initialize a list to store label time courses\n",
    "        input('pause')\n",
    "        print(stcs_lh, stcs_rh)\n",
    "        for idx, (stc_lh, stc_rh) in enumerate(zip(stcs_lh, stcs_rh)):\n",
    "            try:\n",
    "                label_tc_lh = stc_lh.extract_label_time_course(\n",
    "                    labels, src=src, mode='mean_flip')\n",
    "                label_tc_rh = stc_rh.extract_label_time_course(\n",
    "                    labels, src=src, mode='mean_flip')\n",
    "                label_time_courses.extend([label_tc_lh, label_tc_rh])\n",
    "                print(src)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting label time courses for iteration {idx}: {e}\")\n",
    "        else:  # This block will execute if the for loop completes without encountering a break statement\n",
    "            print(\"All time courses have been successfully extracted!\")\n",
    "\n",
    "        # Convert label_time_courses to a NumPy array\n",
    "        label_time_courses_np = np.array(label_time_courses)\n",
    "\n",
    "        # If you prefer to save as a .csv file\n",
    "        # Convert to DataFrame and save as .csv\n",
    "        # label_time_courses_df = pd.DataFrame(label_time_courses_np)\n",
    "        # label_time_courses_df.to_csv(os.path.join(output_dir, f\"{subj}_label_time_courses.csv\"), index=False)\n",
    "\n",
    "        # Save the label time courses as a .npy file\n",
    "        # Replace with your desired output directory\n",
    "        label_time_courses_file = output_path + f\"{subject}_label_time_courses.npy\"\n",
    "        \n",
    "        np.save(label_time_courses_file, label_time_courses_np)\n",
    "\n",
    "        ########################################################################################################################\n",
    "\n",
    "        # Plotting Time Courses\n",
    "        random_idx = np.random.randint(len(label_time_courses))\n",
    "        random_time_course = label_time_courses[random_idx]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(random_time_course)\n",
    "        plt.title(f'Time Course for Randomly Selected Region: {random_idx}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        \n",
    "\n",
    "        plt.savefig(output_path+'time_courses.png')\n",
    "\n",
    "        # Connectivity Visualization for left hemisphere\n",
    "        num_regions = len(label_time_courses[0])\n",
    "        connectivity_matrix = np.zeros((num_regions, num_regions))\n",
    "\n",
    "        for i in range(num_regions):\n",
    "            for j in range(num_regions):\n",
    "                connectivity_matrix[i, j], _ = pearsonr(\n",
    "                    label_time_courses[0][i], label_time_courses[0][j])\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(connectivity_matrix, cmap='viridis', origin='lower')\n",
    "        plt.title('Connectivity Matrix')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Region')\n",
    "        plt.colorbar(label='Pearson Correlation')\n",
    "        \n",
    "\n",
    "        plt.savefig(output_path+'connectivity_matrix.png')\n",
    "\n",
    "        # Average connectivity matrix across all epochs and hemispheres\n",
    "        # Initialize connectivity matrix\n",
    "        num_epochs_hemispheres = len(label_time_courses)\n",
    "        num_regions = label_time_courses[0].shape[0]\n",
    "        all_connectivity_matrices = np.zeros(\n",
    "            (num_epochs_hemispheres, num_regions, num_regions))\n",
    "\n",
    "        # Compute connectivity for each epoch and hemisphere\n",
    "        for k in range(num_epochs_hemispheres):\n",
    "            for i in range(num_regions):\n",
    "                for j in range(num_regions):\n",
    "                    all_connectivity_matrices[k, i, j], _ = pearsonr(\n",
    "                        label_time_courses[k][i], label_time_courses[k][j])\n",
    "\n",
    "        # Average across all epochs and hemispheres\n",
    "        avg_connectivity_matrix = np.mean(all_connectivity_matrices, axis=0)\n",
    "\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(avg_connectivity_matrix, cmap='viridis', origin='lower')\n",
    "        plt.title('Average Connectivity Matrix')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Region')\n",
    "        plt.colorbar(label='Pearson Correlation')\n",
    "        \n",
    "\n",
    "        plt.savefig(output_path+'connectivity_avg_matrix.png')\n",
    "\n",
    "        ########################################################################################################################\n",
    "        # All-to-all connectivity analysis\n",
    "\n",
    "\n",
    "        # Load the label time courses\n",
    "        label_time_courses_file = output_path+ f\"{subject}_label_time_courses.npy\"\n",
    "\n",
    "        label_time_courses = np.load(label_time_courses_file)\n",
    "\n",
    "        # Load labels from the atlas\n",
    "        labels = mne.read_labels_from_annot('fsaverage', parc='Schaefer2018_100Parcels_7Networks_order',\n",
    "                                            subjects_dir=r'../data/in/')\n",
    "\n",
    "        # Group labels by network\n",
    "        networks = {}\n",
    "        for label in labels:\n",
    "            # Extract network name from label name (assuming format: 'NetworkName_RegionName')\n",
    "            network_name = label.name.split('_')[0]\n",
    "            if network_name not in networks:\n",
    "                networks[network_name] = []\n",
    "            networks[network_name].append(label)\n",
    "\n",
    "        # Organize regions by their network affiliations and extract the desired naming convention\n",
    "        ordered_regions = []\n",
    "        network_labels = []  # This will store the network each region belongs to\n",
    "\n",
    "        for label in labels:\n",
    "            # Extract the desired naming convention \"PFCl_1-lh\" from the full label name\n",
    "            parts = label.name.split('_')\n",
    "            region_name = '_'.join(parts[2:])\n",
    "            ordered_regions.append(region_name)\n",
    "\n",
    "            # Extract the network name and store it in network_labels\n",
    "            network_name = parts[2]\n",
    "            network_labels.append(network_name)\n",
    "\n",
    "        # Compute cross-correlation between all pairs of regions across windows\n",
    "\n",
    "        # Time-resolved dPLI computation\n",
    "        sampling_rate = 512  # in Hz\n",
    "        window_length_seconds = 1\n",
    "        step_size_seconds = 0.5\n",
    "\n",
    "        # Total duration in samples\n",
    "        # Assuming the structure is the same as label_time_courses in Code 2\n",
    "        num_epochs_per_hemisphere = label_time_courses.shape[0] / 2\n",
    "        duration_per_epoch = label_time_courses.shape[2] / sampling_rate\n",
    "        total_duration_samples = int(\n",
    "            num_epochs_per_hemisphere * duration_per_epoch * sampling_rate)\n",
    "\n",
    "        window_length_samples = int(window_length_seconds * sampling_rate)\n",
    "        step_size_samples = int(step_size_seconds * sampling_rate)\n",
    "\n",
    "        num_windows = int(\n",
    "            (total_duration_samples - window_length_samples) / step_size_samples) + 1\n",
    "        windowed_dpli_matrices = []\n",
    "        windowed_cross_correlation_matrices = []\n",
    "\n",
    "        for win_idx in range(num_windows):\n",
    "            start_sample = win_idx * step_size_samples\n",
    "            end_sample = start_sample + window_length_samples\n",
    "\n",
    "            # Check if end_sample exceeds the total number of samples\n",
    "            if end_sample > total_duration_samples:\n",
    "                break\n",
    "\n",
    "            # Calculate epoch and sample indices\n",
    "            start_epoch = start_sample // label_time_courses.shape[2]\n",
    "            start_sample_in_epoch = start_sample % label_time_courses.shape[2]\n",
    "            end_epoch = end_sample // label_time_courses.shape[2]\n",
    "            end_sample_in_epoch = end_sample % label_time_courses.shape[2]\n",
    "\n",
    "            # Extract data across epochs\n",
    "            if start_epoch == end_epoch:\n",
    "                windowed_data = label_time_courses[start_epoch,\n",
    "                                                :, start_sample_in_epoch:end_sample_in_epoch]\n",
    "            else:\n",
    "                first_part = label_time_courses[start_epoch, :, start_sample_in_epoch:]\n",
    "                samples_needed_from_second_epoch = window_length_samples - \\\n",
    "                    first_part.shape[1]\n",
    "                second_part = label_time_courses[end_epoch,\n",
    "                                                :, :samples_needed_from_second_epoch]\n",
    "                windowed_data = np.concatenate((first_part, second_part), axis=1)\n",
    "\n",
    "            # dPLI computation\n",
    "            dpli_result = compute_dPLI(windowed_data)\n",
    "            windowed_dpli_matrices.append(dpli_result)\n",
    "\n",
    "            # Cross-correlation computation\n",
    "            cross_corr_result = compute_cross_correlation(windowed_data)\n",
    "            windowed_cross_correlation_matrices.append(cross_corr_result)\n",
    "\n",
    "        # Check the number of windows in the list\n",
    "        num_of_windows = len(windowed_dpli_matrices)\n",
    "        print(f\"Total number of windows: {num_of_windows}\")\n",
    "\n",
    "        # For visualization, select one of the windows\n",
    "        chosen_window = 3\n",
    "        dPLI_matrix = windowed_dpli_matrices[chosen_window]\n",
    "        CrossCorr_matrix = windowed_cross_correlation_matrices[chosen_window]\n",
    "\n",
    "        G_dPLI = nx.convert_matrix.from_numpy_array(dPLI_matrix)\n",
    "        G_dPLI_thresholded = disparity_filter(G_dPLI)\n",
    "\n",
    "        G_CrossCorr = nx.convert_matrix.from_numpy_array(CrossCorr_matrix)\n",
    "        G_CrossCorr_thresholded = disparity_filter(G_CrossCorr)\n",
    "\n",
    "        # Visualization using dPLI\n",
    "        node_angles = np.linspace(0, 360, len(ordered_regions), endpoint=False)\n",
    "        # Initialize a matrix of zeros with the same shape as dPLI_matrix\n",
    "        dPLI_matrix_thresholded = np.zeros_like(dPLI_matrix)\n",
    "        # Iterate through the edges of the thresholded graph\n",
    "        for i, j, data in G_dPLI_thresholded.edges(data=True):\n",
    "            dPLI_matrix_thresholded[i, j] = data['weight']\n",
    "            dPLI_matrix_thresholded[j, i] = data['weight']\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), facecolor='black',\n",
    "                            subplot_kw=dict(polar=True))\n",
    "        plot_connectivity_circle(dPLI_matrix_thresholded, ordered_regions, n_lines=300, node_angles=node_angles,\n",
    "                                title='Thresholded Regional Connectivity using dPLI', ax=ax)\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(output_path+'dpli_viz.png')\n",
    "\n",
    "        # Visualization using Cross-Correlation for a chosen window\n",
    "\n",
    "        # Convert the graph to a matrix function\n",
    "\n",
    "\n",
    "\n",
    "        # Using the above function, convert thresholded graph to matrix\n",
    "        CrossCorr_thresholded_matrix = graph_to_matrix(\n",
    "            G_CrossCorr_thresholded, CrossCorr_matrix.shape[0])\n",
    "\n",
    "        # Visualization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Using the provided plot_matrix function\n",
    "        plot_matrix(CrossCorr_thresholded_matrix,\n",
    "                    f'Thresholded Cross-Correlation Matrix for Window {chosen_window}', ordered_regions)\n",
    "\n",
    "        # Dynamic representation (animation) across all windows for heatmat\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        sns.heatmap(graph_to_matrix(G_CrossCorr_thresholded,\n",
    "                    CrossCorr_matrix.shape[0]), cmap='viridis', xticklabels=ordered_regions, yticklabels=ordered_regions, ax=ax)\n",
    "        title = ax.set_title(f'Thresholded Cross-Correlation Matrix for Window 0')\n",
    "        plt.savefig('heatmap.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ani = FuncAnimation(fig, update, frames=num_of_windows, blit=True)\n",
    "\n",
    "        # Dynamics representation (animation) across all windows from circular graph\n",
    "\n",
    "        node_angles = np.linspace(0, 360, len(ordered_regions), endpoint=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), facecolor='black',\n",
    "                            subplot_kw=dict(polar=True))\n",
    "        plot_connectivity_circle(threshold_matrix(windowed_dpli_matrices[0]), ordered_regions, n_lines=300,\n",
    "                                node_angles=node_angles,\n",
    "                                title='Thresholded Regional Connectivity using dPLI for Window 0', ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ani = FuncAnimation(fig, update, frames=num_of_windows, blit=True)\n",
    "        plt.savefig(output_path+'funcanim.png')\n",
    "\n",
    "\n",
    "        # Histogram of Connectivity Values for the chosen window using dPLI\n",
    "        plt.hist(dPLI_matrix.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "        plt.title('Distribution of dPLI Connectivity Values')\n",
    "        plt.xlabel('Connectivity Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(output_path+'histogram.png')\n",
    "\n",
    "        # Histogram of Connectivity Values for the chosen window using Cross-Correlation\n",
    "        plt.hist(CrossCorr_matrix.flatten(), bins=50, color='red', alpha=0.7)\n",
    "        plt.title('Distribution of Cross-Correlation Connectivity Values')\n",
    "        plt.xlabel('Connectivity Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(output_path+'cross.png')\n",
    "        ########################################################################################################################\n",
    "\n",
    "        # Next --> fc between groups and minimum spanning tree to examine nature of the fc difference between groups"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
